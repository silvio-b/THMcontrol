import pyEp
import os
import time as tm
import numpy as np
import pandas as pd
import os
from Agents.TabQ import TabularQLearning

q_table_cold = np.load(r'C:\Users\LUCA SANDRI\Desktop\Tesi\000_PRATICA\Outputs\cold_table.npy')
q_table_hot = np.load(r'C:\Users\LUCA SANDRI\Desktop\Tesi\000_PRATICA\Outputs\hot_table.npy')

import pyEp
import os
import time as tm
import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt
import pickle
from Agents.TabQ import TabularQLearning

pyEp.set_eplus_dir("C:\\EnergyPlusV9-2-0")

directory = os.path.dirname(os.path.realpath(__file__))

path_to_buildings = os.path.join(directory)
# 'C:\\Users\\SilvioBrandi\\Desktop'
builder = pyEp.socket_builder(path_to_buildings)
configs = builder.build()  # Configs is [port, building_folder_path, idf]
weather_file = 'GBR_London.Gatwick.037760_IWEC'
ep = pyEp.ep_process('localhost', configs[0][0], configs[0][1], weather_file)

#	Cosimulation
outputs = []

EPTimeStep = 1
SimDays = 365
kStep = 0
MAXSTEPS = int(SimDays * 24 * EPTimeStep) + 1
deltaT = (60 / EPTimeStep) * 60

# TabQ Learning Parameters
learning_rate = 1                                       ### Structural parameter
gamma = 0.99                                            ### Structural parameter
n_actions = 4                                           ### Structural parameter
temperature = 1
season = 0

# Process first output
output = ep.decode_packet_simple(ep.read())
I_Dir = output[3]
I_Diff = output[4]
state = I_Dir + I_Diff
state_index = int(np.ceil(state/200))       #dicretizza lo stato

n_states = 6

BCVTB_THM_CONTROL = 3
inputs = [BCVTB_THM_CONTROL + 1]
input_packet = ep.encode_packet_simple(inputs, 0)
ep.write(input_packet)
kStep = kStep + 1
season=0


TQL = TabularQLearning(lr=learning_rate,
                       gamma=gamma,
                       eps=0,
                       temperature=temperature,
                       action_space_shape=n_actions,
                       state_space_shape=n_states,
                       season=season)

TQL.q_table_cold = q_table_cold
TQL.q_table_hot = q_table_hot



first_step_hot = 24 * (31+28+31+30) + 1                     #prima ora di Maggio
last_step_hot = first_step_hot + 24 * (31+30+31+31+30)      #prima ora di Ottobre

rewards = []


while kStep < MAXSTEPS:
    time = (kStep - 1) * deltaT
    dayTime = time % 86400
    if dayTime == 0:
        print(kStep)

    output = ep.decode_packet_simple(ep.read()) # columns = ['Time','Day','OutdoorTemp', 'DirectRad', 'DiffuseRad', 'ZoneTemp', 'Occupancy',
                                                #           'EPh', 'EPc', 'EPl', 'ECwindow']
    TIME = output[0]
    DAY = output[1]
    I_Dir = output[3]
    I_Diff = output[4]
    next_state = I_Dir + I_Diff

    next_state_index = int(np.ceil(next_state / 200))

    reward = - output[7] - output[8] - output[9]
    rewards.append(reward)

    BCVTB_THM_CONTROL = TQL.get_greedy_action(next_state_index)
    inputs = [BCVTB_THM_CONTROL + 1]
    input_packet = ep.encode_packet_simple(inputs, time)
    ep.write(input_packet)

    state_index = next_state_index

    kStep = kStep + 1

tm.sleep(10)
ep.close()

import pickle
with open(r'C:\Users\LUCA SANDRI\Desktop\Tesi\000_PRATICA\Outputs\rewards', "wb") as fp:   #Pickling
    pickle.dump(rewards, fp)


print('\n'.join(['    '.join(["{:.5f}".format(item) for item in row])
      for row in TQL.q_table_cold]))
print('\n'.join(['    '.join(["{:.5f}".format(item) for item in row])
      for row in TQL.q_table_hot]))